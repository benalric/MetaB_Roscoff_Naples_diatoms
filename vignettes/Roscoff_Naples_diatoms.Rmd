---
title: "DADA2 pipeline with MiSeq dataset (18S rDNA) for Roscoff and Naples time series"
author: "Benjamin Alric"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{color, fancyvrb}
output:
  rmdformats::readthedown:
    highlight: tango
    number_section: yes
    css: custom.css
---

```{r setup, include = FALSE}
# Global options
options(max.print = "100")
knitr::opts_chunk$set(echo = TRUE,
               eval = FALSE,
	             cache = TRUE,
               prompt = FALSE,
               tidy = TRUE,
               comment = NA,
               message = FALSE,
               warning = FALSE)
knitr::opts_knit$set(width = 100)
library(DiagrammeR)
```

# Aims
In this repository, you will find a tutorial explaining how we processed metabarcoding data (rDNA 18S V4 region) to report on the genetic diversity of diatom communities in two coastal sites with contrasting hydrological features but recurring annual sequences of plankton species: the SOMLIT-Astan and LTER-Marechiara stations, located respectively in Roscoff (France, Western English Channel) and Naples (Italy, Mediterranean Sea, Gulf of Naples).

The objective was to generate tables of amplicon sequence variants (i.e. ASVs) to study the temporal dynamics of diatom communities at both sites.

For this, we used a pipeline based on the [DADA2 suite](https://www.nature.com/articles/nmeth.3869) as implemented in [R](https://benjjneb.github.io/dada2/dada-installation.html).

# Overview of our bioinformatic pipeline
The pipeline is composed to three steps according to the following workflow.
```{r, eval = TRUE, echo = FALSE}
DiagrammeR::mermaid("
graph TD
A(FastQ sequencing R1 and R2 files) --> B(Step 1 - Filtering and Trimming)
B --> C(FWD and REV primer trimmed sequence for each sample)
C --> D(Step 2 - DADA2: Filtration and length trimming, denoising and merging, chimera detection and ASV table construction)
D --> E(Amplicon Sequence Variants)
E --> F(Step 3 - Taxonomic assignment)
style A fill:#d9e6eb,stroke:#333333,stroke-width:2px
style B fill:#F0E442,stroke:#333333,stroke-width:2px
style C fill:#d9e6eb,stroke:#333333,stroke-width:2px
style D fill:#F0E442,stroke:#333333,stroke-width:2px
style E fill:#d9e6eb,stroke:#333333,stroke-width:2px
style F fill:#F0E442,stroke:#333333,stroke-width:2px
", height = 600)
```
# Step 1 - Filtering and Trimming
## Data demultiplexing
**DADA2** pipeline requires one FASTQ file per sample (or two FASTQ files, one forward (FWD) and one reverse (REV) per sample). Demultiplexing refers to the step of processing where tags are used to find out which sequences came from which samples after they have all been sequenced together.

If there are two FASTQ files (one forward, and one reverse) per sample, then demultiplexing has already been done.
<br/>That was the case for samples of the LETER-Marechiara station.
<br/>In the other case, demultiplexing must be carried out first, as for some runs of the SOMLT-Astan station.

## Primer trimming
After demultiplexing, primers were trimmed using Cutadapt v2.8 ([Martin, 2011](https://journal.embnet.org/index.php/embnetjournal/article/view/200)).
<br/>In our case, reads are in mixed orientation (i.e. R1 and R2 files) so Cutadapt is used twice: to search for tags and forward primers firstly in R1 file and then in R2 file.

## In practice
To perform this first step, a text file (**manifest.txt**) in which are listed the samples, the corresponding tags and the path to the raw FASTQ files to be processed is first created by the [A01_a_nodemul.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/A01_a_nodemul.qsub) script:
```{bash}
#!/bin/bash
#SBATCH --job-name=A01_a.qsub                               # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 1                                   # number of CPUs required per task
#SBATCH --mem 4GB                                           # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o A01_a.out                                        # output file
#SBATCH -e A01_a.err                                        # error file

MANIFEST="manifest.txt"
LOG="/shared/projects/marechiara/Astan_18SV4/log/primer_demutrim.log"
echo -e 'sample\tstatus\tin_reads\tin_bp\ttoo_short\ttoo_long\ttoo_many_n\tout_reads\tw/adapters\tqualtrim_bp\tout_bp\tw/adapters2\tqualtrim2_bp\tout2_bp' > ${LOG}
echo -n "" > A01_a.out
echo -n "" > A01_a.err
RUNID="ADNU_24"
FORWARD="/shared/projects/marechiara/data/190605_SN1126_A_L001_ADNU-24_R1.fastq.gz"
REVERSE="/shared/projects/marechiara/data/190605_SN1126_A_L001_ADNU-24_R2.fastq.gz"
awk 'NR>1 {print $2,toupper($4)}' samples.list | \
sed "s/\.//g" | \
awk -v a=$RUNID -v b=$FORWARD -v c=$REVERSE '{print $0,a,b,c}' > $MANIFEST
```
<span style="color:#000000">Table 1: Preview of the file **manisfest.txt** for non demultiplexed samples</span>
```{r, echo = FALSE, eval = TRUE, results = 'asis'}
tabl <- "
| Sample    | Tag       | Run_id  | FWD read                                                                      | REV read                                                                        |
|-----------|-----------|---------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
|RA090107_3 | AACAACAA  | GYJ     | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R1.fastq.gz | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R2.fastq.gz   |
|RA090120_3 | AGCATGCG  | GYJ     | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R1.fastq.gz | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R2.fastq.gz   |
|RA090205_3 | CTTCTTCA  | GYJ     | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R1.fastq.gz | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R2.fastq.gz   |
|RA090218_3 | AACAATGG  | GYJ     | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R1.fastq.gz | /projet/sbr/plancton-astan/archive/GYJ/131129_SN1126_A_L001_GYJ-4_R2.fastq.gz   |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```
For samples already demultiplexed, the following script [A01_a_demul.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/A01_a_demul.qsub) was used to create the text file **manifest.txt**:
```{bash}
#!/bin/bash
#SBATCH --job-name=A01_a.qsub                                     # job name
#SBATCH --mail-type=BEGIN,END                                     # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                          # where to send mail
#SBATCH --cpus-per-task 1                                         # number of CPUs required per task
#SBATCH --mem 4GB                                                 # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Marechiara_18SV4    # set working directory
#SBATCH -p fast                                                   # partition
#SBATCH -o A01_a.out                                              # output file
#SBATCH -e A01_a.err                                              # error file

MANIFEST="manifest.txt"
LOG="/shared/projects/marechiara/Marechiara_18SV4/log/primer_demutrim.log"
echo -e 'sample\tstatus\tin_reads\tin_bp\ttoo_short\ttoo_long\ttoo_many_n\tout_reads\tw/adapters\tqualtrim_bp\tout_bp\tw/adapters2\tqualtrim2_bp\tout2_bp' > ${LOG}
echo -n "" > A01_a.out
echo -n "" > A01_a.err
RUNID="MC"
ls /shared/projects/marechiara/bioinfo/data/raw/marechiara/ | sort | \
grep -E R[12]_001\.fastq\.gz | sed -E "s/_R[12].+$//" | uniq | \
awk -v a="/shared/projects/marechiara/bioinfo/data/raw/marechiara/" -v b=$RUNID \
'{print $1,"NOTAG",b,a$1"_R1_001.fastq.gz",a$S1"_R2_001.fastq.gz"}' | \
awk '{sub(/\-18S-.+$/,"_18S",$1)}1' >> $MANIFEST
```
<span style="color:#000000">Table 2: Preview of the file **manisfest.txt** for demultiplexed samples</span>
```{r, echo = FALSE, eval = TRUE, results = 'asis'}
tabl <- "
| Sample    | Tag       | Run_id  | FWD read                                                                      | REV read                                                                        |
|-----------|-----------|---------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
|MC1003_18S | NOTAG | MC  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1003-18S-V4_S12_L001_R1_001.fastq.gz  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1003-18S-V4_S12_L001_R2_001.fastq.gz  |
|MC1007_18S | NOTAG | MC  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1007-18S-V4_S13_L001_R1_001.fastq.gz  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1007-18S-V4_S13_L001_R2_001.fastq.gz  |
|MC1012_18S | NOTAG | MC  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1012-18S-V4_S14_L001_R1_001.fastq.gz  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1012-18S-V4_S14_L001_R2_001.fastq.gz  |
|MC1014_18S | NOTAG | MC  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1014-18S-V4_S15_L001_R1_001.fastq.gz  | /shared/projects/marechiara/bioinfo/data/raw/marechiara/MC1014-18S-V4_S15_L001_R2_001.fastq.gz  |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

Then the following script [A01_demultiplex_primer_trim.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/A01_demultiplex_primer_trim.qsub) is applied to each sample (`$SLURM_ARRAY_TASK_ID`):
```{bash}
#!/bin/bash
#SBATCH --job-name=A01_demultiplex_primer_trim.qsub         # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 1                                   # number of CPUs required per task
#SBATCH --mem 10GB                                          # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o A01_demultiplex_primer_trim.out                  # output file
#SBATCH -e A01_demultiplex_primer_trim.err                  # error file

module load cutadapt/2.8
# Primers
PRIMER_F="^CCAGCASCYGCGGTAATTCC"
PRIMER_R="^ACTTTCGTTCTTGATYRA"
# Manifest
read SAMPLE TAG RUNID FORWARD REVERSE <<< $(awk "NR==$SLURM_ARRAY_TASK_ID" manifest.txt)
# Primer trimmed files
CUT_F1="FWD/${SAMPLE}_${RUNID}_Cut1_trimmed.fastq.gz"
CUT_F2="FWD/${SAMPLE}_${RUNID}_Cut2_trimmed.fastq.gz"
CUT_R1="REV/${SAMPLE}_${RUNID}_Cut1_trimmed.fastq.gz"
CUT_R2="REV/${SAMPLE}_${RUNID}_Cut2_trimmed.fastq.gz"
OUT_R1="output/${SAMPLE}_${RUNID}_R1.fastq.gz"
OUT_R2="output/${SAMPLE}_${RUNID}_R2.fastq.gz"
# Log file
LOG="/shared/projects/marechiara/Astan_18SV4/log/${SAMPLE}_${RUNID}_primer_demutrim.log"
TMP_LOG=$(mktemp --tmpdir="/shared/projects/marechiara/Astan_18SV4/scratch")
TMP_TAG_F=$(mktemp --tmpdir="/shared/projects/marechiara/Astan_18SV4/scratch/")".fastq.gz"
TMP_TAG_R=$(mktemp --tmpdir="/shared/projects/marechiara/Astan_18SV4/scratch/")".fastq.gz"
# Demultiplexing - R1
cutadapt -g "XNNNN${TAG}" -O ${#TAG} --discard-untrimmed --no-indels -o ${TMP_TAG_F} -p ${TMP_TAG_R} ${FORWARD} ${REVERSE}
cat ${TMP_TAG_F} > ${OUT_R1}
cat ${TMP_TAG_R} > ${OUT_R2}
# Primer trimming - R1
cutadapt -g "${PRIMER_F}" -G "${PRIMER_R}" --report=minimal --discard-untrimmed --minimum-length 100 --no-indels -o ${CUT_F1} -p ${CUT_R1} ${TMP_TAG_F} ${TMP_TAG_R}  1> ${TMP_LOG}
awk -v a="${SAMPLE}_${RUNID}_Cut1" 'BEGIN {OFS="\t"}; NR==2{print a,$0}' ${TMP_LOG} > ${LOG}
# Demultiplexing - R2
cutadapt -g "XNNNN${TAG}" -O ${#TAG} --discard-untrimmed --no-indels -o ${TMP_TAG_F} -p ${TMP_TAG_R} ${REVERSE} ${FORWARD}
cat ${TMP_TAG_F} >> ${OUT_R2}
cat ${TMP_TAG_R} >> ${OUT_R1}
# Primer trimming - R2
cutadapt -g "${PRIMER_F}" -G "${PRIMER_R}" --report=minimal --discard-untrimmed --minimum-length 100 --no-indels -o ${CUT_F2} -p ${CUT_R2} ${TMP_TAG_F} ${TMP_TAG_R}  1> ${TMP_LOG}
awk -v a="${SAMPLE}_${RUNID}_Cut2" 'BEGIN {OFS="\t"}; NR==2{print a,$0}' ${TMP_LOG}  >> ${LOG}
```
In this script, the primers need to be specified.<br/>Here, we used as gene marker, the rDNA 18S V4 region and the associated eukaryotic-specific primers developped by [Stoeck et al. (2010)](https://onlinelibrary.wiley.com/doi/10.1111/j.1365-294X.2009.04480.x):
<br/>`PRIMER_F="^CCAGCASCYGCGGTAATTCC"` (TAReuk545FWD1, Saccharomyces cerevisiae position 565-584),
<br/>`PRIMER_R="^ACTTTCGTTCTTGATYRA"` (TAReukRev3, Saccharomyces cerevisiae position 964-981).
<br/>The forward reads are placed in the FWD/ folder, those extracted from the R1 file are named Cut1 and those extracted from the R2 file are named Cut2.
<br/>The reverse files are placed in the REV/ folder and named accordingly to the forward reads.

For the LTER_Marechiara station, samples had already been demultiplexed.<br/>So, we used the [A01_primer_trim.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/A01_primer_trim.qsub) script instead:
```{bash}
#!/bin/bash
#SBATCH --job-name=A01_primer_trim.qsub                           # job name
#SBATCH --mail-type=BEGIN,END                                     # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                          # where to send mail
#SBATCH --cpus-per-task 1                                         # number of CPUs required per task
#SBATCH --mem 10GB                                                # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Marechiara_18SV4    # set working directory
#SBATCH -p fast                                                   # partition
#SBATCH -o A01_primer_trim.out                                    # output file
#SBATCH -e A01_primer_trim.err                                    # error file

module load cutadapt/2.8
# Primers
PRIMER_F="^CCAGCASCYGCGGTAATTCC"
PRIMER_R="^ACTTTCGTTCTTGATYRA"
# Manifest
read SAMPLE TAG RUNID FORWARD REVERSE <<< $(awk "NR==$SLURM_ARRAY_TASK_ID" manifest.txt)
# Primer trimmed files
CUT_F1="FWD/${SAMPLE}_${RUNID}_Cut1_trimmed.fastq.gz"
CUT_F2="FWD/${SAMPLE}_${RUNID}_Cut2_trimmed.fastq.gz"
CUT_R1="REV/${SAMPLE}_${RUNID}_Cut1_trimmed.fastq.gz"
CUT_R2="REV/${SAMPLE}_${RUNID}_Cut2_trimmed.fastq.gz"
OUT_R1="output/${SAMPLE}_${RUNID}_R1.fastq.gz"
OUT_R2="output/${SAMPLE}_${RUNID}_R2.fastq.gz"
# Log file
LOG="/shared/projects/marechiara/bioinfo/analysis/Marechiara_18SV4/log/${SAMPLE}_${RUNID}_primer_demutrim.log"
TMP_LOG=$(mktemp --tmpdir="/shared/projects/marechiara/bioinfo/analysis/Marechiara_18SV4/scratch")
# Primer trimming - R1
cutadapt -g "${PRIMER_F}" -G "${PRIMER_R}" --report=minimal --discard-untrimmed --minimum-length 100 --no-indels -o ${CUT_F1} -p ${CUT_R1} ${FORWARD} ${REVERSE}  1> ${TMP_LOG}
awk -v a="${SAMPLE}_${RUNID}_Cut1" 'BEGIN {OFS="\t"}; NR==2{print a,$0}' ${TMP_LOG} > ${LOG}
# Primer trimming - R2
cutadapt -g "${PRIMER_F}" -G "${PRIMER_R}" --report=minimal --discard-untrimmed --minimum-length 100 --no-indels -o ${CUT_F2} -p ${CUT_R2} ${REVERSE} ${FORWARD}  1> ${TMP_LOG}
awk -v a="${SAMPLE}_${RUNID}_Cut2" 'BEGIN {OFS="\t"}; NR==2{print a,$0}' ${TMP_LOG}  >> ${LOG}
```
The script **A01_demultiplex_primer_trim.qsub** (or **A01_primer_trim.qsub**) is launched consecutively on groups of samples by the following script [A01_a1.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/A01_a1.qsub):
```{bash}
#!/bin/bash
#SBATCH --job-name=A01_a1.qsub                              # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 1                                   # number of CPUs required per task
#SBATCH --mem 10GB                                          # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o A01_a1.out                                       # output file
#SBATCH -e A01_a1.err                                       # error file

RUNID="ADNU_24"
sbatch --array=1-$(cat manifest.txt | wc -l)%50 script/A01_demultiplex_primer_trim.qsub # submit a large number of task jobs
```
# Step 2 - DADA2: Filtration and length trimming, denoising and merging, chimera detection and ASV table construction
**DAD2** pipeline is consisted of a series of steps to filter raw sequences obtained through Illumina sequencing (here MiSeq). The starting point of the **DADA2** pipeline is a set of demultiplexed FASTQ files corresponding to the samples in our amplicon sequencing study. That is, **DADA2** expects there to be an individual fastq file for each sample (or two fastq files, one forward and one reverse for each sample).

Once demultiplexed FASTQ files without non-biological nucleotides are in hand (see Step 1), the **DADA2** pipeline proceeds in eight steps as follow:
<br/>- Step 2.1: Inspect read quality profiles
<br/>- Step 2.2: Filtering and trimming
<br/>- Step 2.3: Learn error rates
<br/>- Step 2.4: Dereplicate
<br/>- Step 2.5: Infer sample composition
<br/>- Step 2.6: Merge paired reads
<br/>- Step 2.7: Make sequence table
<br/>- Step 2.8: Remove chimeras

The output of pipeline is a table of ASVs with rows corresponding to samples and columns to amplicon. In this table, the value of each entry is the number of times ASV was observed in that sample. This table is analogous to the traditional OTU table, except at higher resolution, i.e. exact amplicon sequence variants rather than (usually 97%) clusters of sequencing reads.

## Inspect read quality profiles (step 2.1)
It is important to get a feel for the quality of the data that we are using. To do this, we will plot the quality of samples. These plots summarize the quality of the reads and they allow us to define where the quality of the sequences falls.

Plots are generated using the [dada2_01_quality_plot.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_01_quality_plot.qsub) and [dada2_01_quality_plot.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_01_quality_plot.R) scripts:
**dada2_01_quality_plot.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_01_quality_plot.qsub               # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 1                                   # number of CPUs required per task
#SBATCH --mem 4GB                                           # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_01_quality_plot.out                        # output file
#SBATCH -e dada2_01_quality_plot.err                        # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_01_quality_plot.R
```
**dada2_01_quality_plot.R** script:
```{r, eval = FALSE}
## Some packages must be loaded
library(dada2)
library(magrittr)
library(ggplot2)
## Set up pathway to forward and reverse FASTQ files
pathF <- "/shared/projects/marechiara/Astan_18SV4/FWD" 
pathR <- "/shared/projects/marechiara/Astan_18SV4/REV"
## Create folders for quality profiles
dir.create(paste0(pathF, "/qualityplot"))
dir.create(paste0(pathR, "/qualityplot"))
## Set file paths where quality plots will be stored
filtpathF <- file.path(pathF, "qualityplot") 
filtpathR <- file.path(pathR, "qualityplot")
## Get a list of all FASTQ files in the work directory and separate FWD and REV 
## Forward and reverse FASTQ files have format: 
## SAMPLENAME_Cut1_trimmed.fastq.gz and SAMPLENAME_Cut2_trimmed.fastq.gz
fastqFs <- sort(list.files(pathF, pattern = "fastq.gz"))
fastqRs <- sort(list.files(pathR, pattern = "fastq.gz"))
## Select file with a size above to 1000 bytes
fastqFs <- fastqFs[file.size(file.path(pathF, fastqFs)) > 1000]
fastqRs <- fastqRs[file.size(file.path(pathR, fastqRs)) > 1000]
if (length(fastqFs) != length(fastqRs)) {
  stop("Forward and reverse files do not match.")
}
## Identify the name of the run (here one run: GYJ)
runs <- sub("^RA\\d+_\\d+_([^_]+).+$", "\\1", fastqFs) %>% unique
## Quality plots to the sample level
# FWD
pdf(file.path(filtpathF, "indiv_F_Qplots.pdf"))
for (i in file.path(pathF, fastqFs)[1]) {
  print(plotQualityProfile(i))
}
dev.off()
# REV
pdf(file.path(filtpathR, "indiv_R_Qplots.pdf"))
for (i in file.path(pathR, fastqRs)) {
  print(plotQualityProfile(i))
}
dev.off()
## Version of packages used to build this document
sessionInfo()
```
There is one quality plot per sample.<br/>This is the figure generated at one sample of SOMLIt-Astan station for the forward reads:
![Figure 1](C:/Users/benjamin/Documents/Travail_BA/GitHub_pages/MetaB_Roscoff_Naples_diatoms/figures/indiv_FWD_Qplots_GYJ_18SV4.png)
This is the figure generated at one sample of SOMLIt-Astan station for the revers reads:
![Figure 2](C:/Users/benjamin/Documents/Travail_BA/GitHub_pages/MetaB_Roscoff_Naples_diatoms/figures/indiv_REV_Qplots_GYJ_18SV4.png)
**Description of quality plots:**
<br/>In gray-scale is a heat map of the frequency of each quality score at each base position.
<br/>The median quality score at each position id shown by the green line, and the quartiles of the quality score distribution by the orange lines.
<br/>The red line shown the scaled proportion of reads that extend to at least that position (this is more useful for other sequecing technologies, as Illumina reads are typically all the same length, hence the flat red line).

Based on these profiles, we truncated the forward reads and the reverse reads at the position 210 and 220 respectively for the SOMLIT-Astan station and at the position 220 and 210 respectively for the LTER-Marechiara station.
<br/>We are not cutting more to conserve enough length to allow the overlap between the mix-orientated reads.

## Parameters of the DADA2 pipeline
The inspection of read quality profiles allows to estimate parameters, need for the following steps of the **DADA2** pipeline (i.e. step 2.2 to 2.8), which are stored in a .csv file: [dada2_pipeline_parameters.csv](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_pipeline_parameters.csv).

<span style="color:#000000">Table 3: Preview of the parameters for the SOMLIT-Astan station</span>
```{r, echo = FALSE, eval = TRUE, results = 'asis'}
tabl <- "
|step   | variable      | value | comment
|-------|---------------|-------|--------|
|step01 | TRUNC_FWD     | 210   | NA     |
|step01 | TRUNC_REV     | 220   | NA     |
|step01 | MAXEE         | 2     | NA     |
|step02 | MIN_READ_NUM  | 1000  | NA     |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```
<span style="color:#000000">Table 4: Preview of the parameters for the LTER-Marechiara station</span>
```{r, echo = FALSE, eval = TRUE, results = 'asis'}
tabl <- "
|step   | variable      | value | comment
|-------|---------------|-------|--------|
|step01 | TRUNC_FWD     | 220   | NA     |
|step01 | TRUNC_REV     | 210   | NA     |
|step01 | MAXEE         | 2     | NA     |
|step02 | MIN_READ_NUM  | 1000  | NA     |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```
This table allows to tuning these parameters without having to change the scripts of the following steps of the **DADA2** pipeline

## Filtering and trimming (Step 2.2)
In sequence data, low-quality sequences can contain unexpected and misleading errors, and Illumina sequencing quality tends to drop off at the end of reads. Therefore, a step of filtering and trimming is necessary.

This step is achieved by using the function `filterAndTrim()` from the library `dada2`, by setting the trim positions of the forward and reverse reads with the `TRUNC_FWD` and `TRUNC_REV` parameters in the `truncLen` argument of the function. Reads with ambiguous nucleotides or with a maximum number of expected errors greater than a chosen threshold (`MAXEE`) were filtered out based on the set value of the `maxEE` argument.

The [dada2_02_filter_trim_a.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_02_filter_trim_a.qsub) script is first executed to launch the [dada2_02_filter_trim_a.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_02_filter_trim_a.R) script.
**dada2_02_filter_trim_a.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dad2_02_filter_trim_a.qsub               # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 4                                   # number of CPUs required per task
#SBATCH --mem 10GB                                          # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_02_filter_trim_a.out                       # output file
#SBATCH -e dada2_02_filter_trim_a.err                       # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_02_filter_trim_a.R
```
**dada2_02_filter_trim_a.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(dada2)
library(data.table)
library(magrittr)
## Define the following path variable so that it points to the extracted directory
path <- "/shared/projects/marechiara/Astan_18SV4"
pathF <- "/shared/projects/marechiara/Astan_18SV4/FWD" 
pathR <- "/shared/projects/marechiara/Astan_18SV4/REV"
## Create folders for dada2 results
dir.create((paste0(path, "/dada2/log")))
## Load the table of dada2 parameters
dada2_param <- read.csv2(paste0(path, "/dada2_pipeline_parameters.csv"), header = TRUE, stringsAsFactors = FALSE)
## Read in the names of the fastq files 
## Forward and reverse fastq files have format: SAMPLENAME_Cut1_trimmed.fastq.gz and SAMPLENAME_Cut1_trimmed.fastq.gz
fastqFs <- sort(list.files(pathF, pattern = "fastq.gz"))
fastqRs <- sort(list.files(pathR, pattern = "fastq.gz"))
## Select file with a size above to 1000 bytes
fastqFs <- fastqFs[file.size(file.path(pathF, fastqFs)) > 1000]
fastqRs <- fastqRs[file.size(file.path(pathR, fastqRs)) > 1000]
if (length(fastqFs) != length(fastqRs)) {
  stop("Forward and reverse files do not match.")
}
## Arguments for filter and trim
args <- dada2_param[dada2_param$step == "step01", ]
trunc_fwd <- args[args$variable == "TRUNC_FWD", 3] %>% as.numeric
trunc_rev <- args[args$variable == "TRUNC_REV", 3] %>% as.numeric
maxee <- args[args$variable == "MAXEE", 3] %>% as.numeric
## Create path towards directory where filtered results will be stored
pathF <- "/shared/projects/marechiara/Astan_18SV4/FWD" 
pathR <- "/shared/projects/marechiara/Astan_18SV4/REV"
filtpathF <- file.path(pathF, "filtered") 
filtpathR <- file.path(pathR, "filtered")
## Filtering and trimming
for (i in fastqFs) {
  dada2::filterAndTrim(fwd = file.path(pathF, i), filt = file.path(filtpathF, i),
                                     rev = file.path(pathR, i), filt.rev = file.path(filtpathR, i), 
                       truncLen = c(trunc_fwd, trunc_rev),
                       maxEE = maxee, maxN = 0, compress = TRUE, verbose = TRUE, multithread = TRUE) -> out
  
  # Save filtered and trimmed table in the folder of dada2 results
  data.table(out, keep.rownames = TRUE) %>% 
    setnames(names(data.table(out, keep.rownames = TRUE))[1], c("sample")) %>% 
    fwrite(paste0(path, "/dada2/log/filter_", sub("_trimmed.+$", "", basename(i)), ".csv"), sep = ";", col.names = TRUE)
rm(out)
}
## Version of packages used to build this document
sessionInfo()
```

The [dada2_02_filter_trim_a.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_02_filter_trim_a.R) script produces for each sample one file .csv.
<br/>Therefore, the [dada2_02_filter_trim_b.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_02_filter_trim_b.qsub) and [dada2_02_filter_trim_b.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_02_filter_trim_b.R) were launched to merge all files in one table with rows as samples and three columns corresponding to sample names, reads.in, reads.out.
**dada2_02_filter_trim_b.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_02_filter_trim_b.qsub              # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 1                                   # number of CPUs required per task
#SBATCH --mem 4GB                                           # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_02_filter_trim_b.out                       # output file
#SBATCH -e dada2_02_filter_trim_b.err                       # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_02_filter_trim_b.R
```
**dada2_02_filter_trim_b.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(data.table)
library(magrittr)
## Merge all files in one table
path <- "/shared/projects/marechiara/Astan_18SV4/dada2/log"
myfiles <- list.files(path = path, pattern = "*.csv", full.names = TRUE)
filter <- lapply(myfiles, read.csv2)
filter <- do.call('rbind', filter)
filter %>% 
  data.table() %>%
  fwrite(paste0(path, "/filter.csv"), sep = ";", col.names = TRUE)
## Version of packages used to build this document
sessionInfo()
```
## Learn error rates (step 2.3)
Errors can be introduced by PCR amplification and sequencing. Therefore, error rates must be estimate from a learning process in which a subset of our data as a training set. 

This step is done by using the function `learnErrors()` from the library `dada2`. This function learns an error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The error parameters typically vary between sequencing runs and PCR protocols, so this method provides a way to estimate those parameters from the data itself.

The [dada2_03_learn_error.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_03_learn_error.qsub) script is first executed to launch the [dada2_03_learn_error.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_03_learn_error.R) script.
**dada2_03_learn_error.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_03_learn_error.qsub                # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 6                                   # number of CPUs required per task
#SBATCH --mem 10GB                                          # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_03_learn_error.out                         # output file
#SBATCH -e dada2_03_learn_error.err                         # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_03_learn_error.R
```
**dada2_03_learn_error.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(dada2)
library(data.table)
library(foreach)
library(magrittr)
## Load the table of dada2 parameters
path <- "/shared/projects/marechiara/Astan_18SV4"
dada2_param <- read.csv2(paste0(path, "/dada2_pipeline_parameters.csv"), header = TRUE, stringsAsFactors = FALSE)
## Define the following path variable so that it points to the extracted directory
pathF <- "/shared/projects/marechiara/Astan_18SV4/FWD" 
pathR <- "/shared/projects/marechiara/Astan_18SV4/REV"
## Arguments for denoising and merging
args <- dada2_param[dada2_param$step == "step02", ]
min_read_nb <- args[args$variable == "MIN_READ_NUM", 3]
## Track reads through the pipeline
## As a final check of our progress, we'll look at the number of reads that made it through each step in the pipeline
getN <- function(x) sum(getUniques(x))
getA <- function(x) length(getUniques(x))
## selection based on the number of reads
tmp <- fread(paste0(path, "/dada2/log/filter.csv"))
tmpR <- tmpF <- tmp[reads.out >= min_read_nb, sample]
## Identify the name of the run (here one run: MC)
runs <- "MC"
runs <- c(paste(runs, "Cut1", sep = "_"))
## File parsing
filtpathF <- file.path(pathF, "filtered") 
filtpathR <- file.path(pathR, "filtered")
filtFsall <- paste(filtpathF, tmpF, sep = "/")
filtRsall <- paste(filtpathR, tmpR, sep = "/")
sample.names.all <- sub("_trimmed.+$", "", basename(filtFsall))
sample.namesR.all <- sub("_trimmed.+$" , "", basename(filtRsall))
if(!identical(sample.names.all, sample.namesR.all)) {
  stop("Forward and reverse files do not match.")
}
names(filtFsall) <- sample.names.all
names(filtRsall) <- sample.names.all
## set seed to ensure that randomized steps are replicable
set.seed(100)
## Loop allowing to determinate learn error rates and infer sample composition
filtFs <- filtFsall
filtRs <- filtRsall
sample.names <- sub("_trimmed.+$", "", basename(filtFs))
## Learn forward error rates
errF <- learnErrors(filtFs, nbases = 1e8, multithread = FALSE)
pdf(paste0("dada2/log/errF_", runs, ".pdf"))
print(plotErrors(errF, nominalQ = TRUE))
dev.off()
## Learn reverse error rates
errR <- learnErrors(filtRs, nbases = 1e8, multithread = FALSE)
pdf(paste0("dada2/log/errR_", runs, ".pdf"))
print(plotErrors(errR, nominalQ = TRUE))
dev.off()
save(errF, file = paste0(path, "/dada2/log/errF_", runs,".rda")) 
save(errR, file = paste0(path, "/dada2/log/errR_", runs,".rda")) 
## Version of packages used to build this document
sessionInfo()
```
There is one plot for forward and reverse reads.<br/>This is the figure generated at SOMLIt-Astan station for the forward reads:
![Figure 3](C:/Users/benjamin/Documents/Travail_BA/GitHub_pages/MetaB_Roscoff_Naples_diatoms/figures/errF_GYJ_Cut1.png)
This is the figure generated at SOMLIt-Astan station for the reverse reads:
![Figure 3](C:/Users/benjamin/Documents/Travail_BA/GitHub_pages/MetaB_Roscoff_Naples_diatoms/figures/errR_GYJ_Cut1.png)
**Description of error rate plots:**
<br/>The red line represents what the learned error rates should look like for each of the 16 possible base transitions (A->A, A->C, A->G, etc.).
<br/>Grey dots are the observed error rates for each consensus quality score and black lines indicate the error rates expected under the nominal definition of the Q-score.
<br/>The expected error rates should have a good fit with the observed error rates, and the error rates must drop with increased quality.
<br/>If black lines and red lines are very far off from each other, it may be a good idea to increase the `nbases` parameter in the function `learnErrors()`.
<br/>This allows the machine learning algorithm to train on a larger portion of data and may help improve the fit.

## Denoising, and merging (steps 2.4 to 2.7)
After estimating the error rates, each sample was then dereplicated, i.e. strictly identical reads were merged using the function `derepFastq()`. This step allows to condense the data and significantly reduce subsequent computation times.

After dereplication of each sample, the core algorithm of **DADA2** is applied with [dada2_04_denoising_merging.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_04_denoising_merging.qsub) and [dada2_04_denoising_merging.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_04_denoising_merging.R) scripts to infer the true sequence variants.
<br/>For each run, the samples are pooled together by setting the argument `pool = TRUE` in the function `dada()`, which can increase the sensitivity to rare variants.
<br/>Then the forward and reverse reads were merged together by using the function `mergePairs()`.
**dada2_04_denoising_merging.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_04_denoising_merging.qsub          # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 6                                   # number of CPUs required per task
#SBATCH --mem 10GB                                          # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_04_denoising_merging.out                   # output file
#SBATCH -e dada2_04_denoising_merging.err                   # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_04_denoising_merging.R
```
**dada2_04_denoising_merging.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(dada2)
library(data.table)
library(doParallel)
library(foreach)
library(magrittr)
registerDoParallel(cores = 6)
## Load the table of dada2 parameters
path <- "/shared/projects/marechiara/Astan_18SV4"
dada2_param <- read.csv2(paste0(path, "/dada2_pipeline_parameters.csv"), header = TRUE, stringsAsFactors = FALSE)
## Define the following path variable so that it points to the extracted directory
pathF <- "/shared/projects/marechiara/Astan_18SV4/FWD" 
pathR <- "/shared/projects/marechiara/Astan_18SV4/REV"
pathErr <- "/shared/projects/marechiara/Astan_18SV4/dada2/log"
## Arguments for denoising and merging
args <- dada2_param[dada2_param$step == "step02", ]
min_read_nb <- args[args$vairable == "MIN_READ_NUM", 3]
## Track reads through the pipeline
## As a final check of our progress, we'll look at the number of reads that made it through each step in the pipeline
getN <- function(x) sum(getUniques(x))
getA <- function(x) length(getUniques(x))
## selection based on the number of reads
tmp <- fread(paste0(path, "/dada2/log/filter.csv"))
tmpR <- tmpF <- tmp[reads.out >= min_read_nb, sample]
## Identify the name of the run (here one run: PHYTOPORT)
runs <- "GYJ"
runs <- c(paste(runs, "Cut1", sep = "_"), paste(runs, "Cut2", sep = "_"))
## File parsing
filtpathF <- file.path(pathF, "filtered") 
filtpathR <- file.path(pathR, "filtered")
filtFsall <- paste(filtpathF, tmpF, sep = "/")
filtRsall <- paste(filtpathR, tmpR, sep = "/")
sample.names.all <- sub("_trimmed.+$", "", basename(filtFsall))
sample.namesR.all <- sub("_trimmed.+$" ,"", basename(filtRsall))
if(!identical(sample.names.all, sample.namesR.all)) {
  stop("Forward and reverse files do not match.")
}
names(filtFsall) <- sample.names.all
names(filtRsall) <- sample.names.all
filtErr <- sort(list.files(pathErr, pattern = ".rda"))
## set seed to ensure that randomized steps are replicatable
set.seed(100)
## Loop allowing to determinate learn error rates and infer sample composition
foreach(i = runs, .packages = c("data.table","dada2")) %dopar% {
  filtFs <- filtFsall[grep(i, names(filtFsall))]
  filtRs <- filtRsall[grep(i, names(filtRsall))]
  sample.names <- sub("_trimmed.+$","", basename(filtFs))
  # Select the FWD and REV error files
  err <- filtErr[grep(i, filtErr)]
  load(paste(pathErr, err[grep("errF", err)], sep = "/"))
  load(paste(pathErr, err[grep("errR", err)], sep = "/"))
  # Sample inference and merger of paired-end reads
  mergers <- vector("list", length(sample.names))
  names(mergers) <- sample.names
  track <- vector("list", length(sample.names))
  names(track) <- sample.names
  for(sam in sample.names) {
    cat("Processing:", sam, "\n")
    # Dereplication of forward reads+
    derepF <- derepFastq(filtFs[[sam]])
    # Infer sample composition of forward reads
    ddF <- dada(derepF, err = errF, multithread = FALSE, pool = TRUE)
    # Dereplication of revers reads
    derepR <- derepFastq(filtRs[[sam]])
    # Infer sample composition of reverse reads
    ddR <- dada(derepR, err = errR, multithread = FALSE, pool = TRUE)
    # Merge forward/reverse reads
    merger <- mergePairs(ddF, derepF, ddR, derepR, trimOverhang = TRUE, verbose = TRUE)
    mergers[[sam]] <- merger
    track[[sam]] <- data.table(sample = sam, 
                               denoisedF.read = getN(ddF), denoisedR.read = getN(ddR), merged.read = getN(merger),
                               denoisedF.seq = getA(ddF), denoisedR.seq = getA(ddR), merged.seq = getA(merger))
  }
  rm(derepF); rm(derepR)
  # Construct sequence table
  track <- rbindlist(track)
  seqtab <- makeSequenceTable(mergers)
  saveRDS(track, paste0(path, "/dada2/log/track_",i,".rds")) 
  saveRDS(seqtab, paste0(path, "/dada2/seqtab_",i,".rds")) 
  rm(track)
  rm(seqtab)
  for(i in 1:length(mergers)) {
    mergers[[i]] <- data.frame(samples = names(mergers)[i], mergers[[i]])
  }
  mergers <- rbindlist(mergers)
  saveRDS(mergers, paste0(path, "/dada2/log/mergers_",i,".rds"))
}
## Version of packages used to build this document
sessionInfo()
```
## Remove Chimeras (step 2.8)
Although **DADA2** has searched for indel errors and substitutions, there may still be chimeric sequences in our dataset that are another important source of spurious sequences in amplicon sequencing. Therefore, the last step before building the ASV table is to remove chimeras.

Chimeras are sequences that are derived form forward and reverse sequences from two different organisms becoming fused together during PCR and/or sequencing. To identify chimeras, we search for rare sequences variants that can be reconstructed by combining left-hand and right-hand segments from two more abundant "parent" sequences.

This is done with the function `removeBimeraDenovo()` in the [dada2_05_chimeras.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_05_chimeras.R) script, launched by the [dada2_05_chimeras.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_05_chimeras.qsub) script.
<br/>The statistics about the pipeline are put together into one file (**statistics.tsv**) and the ASV is built.
**dada2_05_chimeras.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_05_chimeras.qsub                   # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the begining/end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 6                                   # number of CPUs required per task
#SBATCH --mem 100GB                                         # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH -p fast                                             # partition
#SBATCH -o dada2_05_chimeras.out                            # output file
#SBATCH -e dada2_05_chimeras.err                            # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_05_chimeras.R
```
**dada2_05_chimeras.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(dada2); packageVersion("dada2")
library(data.table)
library(magrittr)
library(digest)
## Define the following path variable so that it points to the extracted directory
pathFilt <- "/shared/projects/marechiara/Astan_18SV4/log"
pathSave <- "/shared/projects/marechiara/Astan_18SV4"
## Merge multiple runs (if necessary)
x <- grep("seqtab_.+\\.rds$", dir("/shared/projects/marechiara/Astan_18SV4/dada2/"), value = TRUE)
x <- paste0("/shared/projects/marechiara/Astan_18SV4/dada2/", x)
st.all <- mergeSequenceTables(tables = x)
## Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method = "pooled", multithread = TRUE, minFoldParentOverAbundance = 2)
seqtab.nochim2 <- seqtab.nochim %>% t %>% data.table
## load statistics
x <- grep("track_.+\\.rds$",dir("/shared/projects/marechiara/Astan_18SV4/log/"), value = TRUE)
x <- paste0("/shared/projects/marechiara/Astan_18SV4/log/", x)
stattab <- lapply(x, readRDS) %>%
  rbindlist
stattab[, nochim.read := sapply(sample, function(X){
  sum(seqtab.nochim2[, get(X)])
})]
stattab[, nochim.seq := sapply(sample, function(X){
  sum(seqtab.nochim2[, get(X)] != 0)
})]
stattab <- stattab[, list(sample, denoisedF.read, denoisedR.read, merged.read, 
                          nochim.read, denoisedF.seq, denoisedR.seq, merged.seq, nochim.seq)]
filter <- read.csv2(paste0(pathFilt, "/filter.csv"), header = TRUE, stringsAsFactors = FALSE)
filter$sample <- sub("_trimmed.+$","", filter$sample)
stattab <- merge(stattab, filter, by.x = "sample", by.y = "sample")
stattab <- stattab[, c("sample", "reads.in", "reads.out",
                       "denoisedF.read", "denoisedR.read", "merged.read", "nochim.read",
                       "denoisedF.seq", "denoisedR.seq", "merged.seq", "nochim.seq")]
fwrite(stattab, "/shared/projects/marechiara/Astan_18SV4/log/statistics.tsv", sep = "\t")
rm(stattab, seqtab.nochim2)
## Construct sequence table
tmp <- seqtab.nochim %>% t %>% data.table(keep.rownames = TRUE)
setnames(tmp, "rn", "sequence")
saveRDS(seqtab.nochim, paste0(pathSave, "/dada2/seqtab.nochim",".rds"))
fwrite(seqtab.nochim, "/shared/projects/marechiara/Astan_18SV4/dada2/seqtab.nochim.tsv", sep = "\t")
rm(seqtab.nochim)
tmp <- data.table(amplicon = sapply(tmp[, sequence], digest, algo = "sha1"), tmp)
fwrite(tmp, "/shared/projects/marechiara/Astan_18SV4/dada2/seqtab_all.tsv", sep = "\t")
## Version of packages used to build this document
sessionInfo()
```
# Step 3 - Taxonomic assignment
At this stage, we have the ASV table which is the final product of the **DADA2** pipeline. In this ASV table each row corresponds to a processed sample, and each column corresponds to a sequence variant. But, the sequences variants are not yet annotated, i.e. assigning a taxonomy to the sequence variants. 

To assign a taxonomy to the phylogenetic marker-gene 18S V4, we used the native implementation of the [naive Bayesian classifer method](https://aem.asm.org/content/73/16/5261.long) provides by the library `dada2`.
<br/>The **PR2 database (v4.13.0)** is used to annotate the ASV table.
<br/>This step is done following the script [dada2_06_AssignTaxo_pr2.R](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_05_chimeras.R) launched by the [dada2_06_AssignTaxo_pr2.qsub](https://github.com/benalric/MetaB_Roscoff_Naples_diatoms/tree/main/script/dada2_05_chimeras.R) script.
**dada2_06_AssignTaxo_pr2.qsub** script:
```{bash}
#!/bin/bash
#SBATCH --job-name=dada2_06_AssignTaxo_pr2.qsub             # job name
#SBATCH --mail-type=BEGIN,END                               # send a mail at the end of job
#SBATCH --mail-user=balric@sb-roscoff.fr                    # where to send mail
#SBATCH --cpus-per-task 6                                   # number of CPUs required per task
#SBATCH --mem 200GB                                         # memory per processor
#SBATCH --workdir=/shared/projects/marechiara/Astan_18SV4   # set working directory
#SBATCH --partition long                                    # partition
#SBATCH --time 2-00:00:00                                   # set the maximum time to 2 days rather than 30 days which is the default
#SBATCH -o dada2_06_AssignTaxo_pr2.out                      # output file
#SBATCH -e dada2_06_AssignTaxo_pr2.err                      # error file

module load r/4.1.0
srun Rscript /shared/projects/marechiara/Astan_18SV4/script/dada2_06_AssignTaxo_pr2.R
```
**dada2_06_AssignTaxo_pr2.R** script:
```{r, eval = FALSE}
## Some packages must be installed and loaded
library(dada2); packageVersion("dada2")
## Define the following path variable so that it points to the extracted directory
pathSeq <- "/shared/projects/marechiara/Astan_18SV4"
pathRef <- "/shared/projects/marechiara/Astan_18SV4/refdb"
## Load sequence table
seqtab.nochim1 <- readRDS(paste0(pathSeq, "/dada2/seqtab.nochim.rds"))
## Pathway to load PR2 reference database
pr2_file <- paste0(pathRef, "/pr2_version_4.13.0_18S_dada2.fasta.gz")
## Fix the taxanomic levels
## PR2 database present specific taxonomic levels
PR2_tax_levels <- c("Kingdom", "Supergroup", "Division", "Class", "Order", "Family", 
                    "Genus", "Species")
## Taxonomic assginment
taxa <- assignTaxonomy(seqtab.nochim1, refFasta = pr2_file, taxLevels = PR2_tax_levels,
                       minBoot = 0, outputBootstraps = TRUE, verbose = TRUE)
save(taxa, file = paste0(pathSeq, "/dada2/seqtab_all_assignTaxo_taxopr2.rda"))
## Version of packages used to build this document
sessionInfo()
```

# Version of packages used to build this document
```{r, eval = TRUE, echo = FALSE}
library(dada2)
library(magrittr)
library(ggplot2)
library(data.table)
library(foreach)
library(doParallel)
library(digest)
sessionInfo()
```